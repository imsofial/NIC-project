{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the needed imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import snntorch as snn\n",
    "import snntorch.spikegen as spikegen\n",
    "import snntorch.functional as SF\n",
    "import snntorch.utils as utils\n",
    "from snntorch import surrogate\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prepocess images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Labels: ['glioma', 'meningioma', 'notumor', 'pituitary']\n"
     ]
    }
   ],
   "source": [
    "# Finding the data in the dictionary\n",
    "data_dir = pathlib.Path(\"..\") / \"brain-tumor-mri-dataset\"\n",
    "train_dir = data_dir / \"Training\"\n",
    "test_dir = data_dir / \"Testing\"\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Convert to grayscale\n",
    "    transforms.Resize((64, 64)),  # Resize for SNN, 64 for saving memory\n",
    "    transforms.RandomRotation(30),  # Rotation augmentation (randomly rotates the image up to Â±30 degrees)\n",
    "    transforms.RandomHorizontalFlip(), # Flips images horizontally (left to right)\n",
    "    transforms.ToTensor(),  # Convert to Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize, by reducing sharp spikes in activation\n",
    "])\n",
    "\n",
    "# Evaluate the model on real, unaltered images, so we dont do image augmantation.\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load images to train and test sets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform_test)\n",
    "\n",
    "# Group multiple images into batches \n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test correctness\n",
    "print(\"Class Labels:\", train_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**encoding images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding transforms each pixel into a series of spikes:\n",
    "\n",
    "# Ensures your code can run on any machine\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define timesteps (how many times an image generates spikes)\n",
    "# Instead of feeding the network one static image,\n",
    "# we feed 20 spike versions of it, to simulate passage of time\n",
    "timesteps = 10\n",
    "\n",
    "# rate-based encoding, it interprets the pixel intensity as a spike probability\n",
    "def encode_batch(batch, timesteps):\n",
    "    return spikegen.rate(batch, num_steps=timesteps).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch.surrogate as surrogate\n",
    "\n",
    "# This class uses spiking neurons, specifically Leaky Integrate-and-Fire(LIF),\n",
    "# to classify brain tumor MRI images into one of four classes\n",
    "class TumorSNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1st convolution layer:\n",
    "        # Input: 1 grayscale channel\n",
    "        # Output: 16 feature maps, using a 5x5 kernel\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
    "        # Leaky Integrate-and-Fire neuron layer after conv1\n",
    "        self.lif1 = snn.Leaky(beta=0.9, spike_grad=surrogate.fast_sigmoid())\n",
    "\n",
    "        # 2nd convolution layer:\n",
    "        # It takes 16 input channels, outputs 32 feature maps\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "        # Leaky Integrate-and-Fire neuron layer after conv2\n",
    "        self.lif2 = snn.Leaky(beta=0.9, spike_grad=surrogate.fast_sigmoid())\n",
    "\n",
    "        # Max pooling layer to reduce spatial dimensions by half\n",
    "        self.pool = nn.MaxPool2d(2) \n",
    "\n",
    "        # Fully connected layer:\n",
    "        # After 2 pools, input image size is 16x16\n",
    "        # With 32 feature maps, total flattened size = 32 * 16 * 16 = 8192\n",
    "        # Output: 4 classes (tumor types)\n",
    "        self.fc = nn.Linear(32 * 16 * 16, 4)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Creates an initial state of that membrane potential, which is required to:\n",
    "        # - Keep track of voltage inside the neuron\n",
    "        # - Allow the model to \"remember\" signals from previous time steps\n",
    "        # for 2 LIF layers\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # To sum up the outputs from each timestep, because:\n",
    "        # x has shape [timesteps, batch, channels, height, width]\n",
    "        spk_sum = 0\n",
    "        for step in range(x.size(0)):  # timesteps\n",
    "            cur = x[step] # Get image spike frame at current timestep\n",
    "\n",
    "            # First conv. + pool layer\n",
    "            cur = self.pool(self.conv1(cur))\n",
    "            spk1, mem1 = self.lif1(cur, mem1) # LIF neuron returns (spike, membrane)\n",
    "\n",
    "            # Second conv + pool layer\n",
    "            cur = self.pool(self.bn1(self.conv2(spk1)))\n",
    "            spk2, mem2 = self.lif2(cur, mem2) # LIF neuron returns (spike, membrane)\n",
    "\n",
    "            # Flatten the output for the fully connected layer, \n",
    "            # turning a 3D tensor (channels, height, width) into a 1D vector\n",
    "            # so it can be passed into a fully connected (dense) layer ????\n",
    "            flat = self.dropout(spk2.view(spk2.size(0), -1))\n",
    "\n",
    "            # Final linear classification layer\n",
    "            out = self.fc(flat)\n",
    "\n",
    "            spk_sum += out\n",
    "\n",
    "        # Return the average output across all timesteps\n",
    "        return spk_sum / x.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**initialize the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TumorSNN().to(device)\n",
    "# Compute class weights for balanced loss\n",
    "targets = [label for _, label in train_dataset]\n",
    "weights = compute_class_weight('balanced', classes=np.unique(targets), y=targets)\n",
    "weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_snn(model, loader, epochs=10):\n",
    "    model.train()  # set the model into training mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0  # Keep track of total loss in this epoch\n",
    "        correct = 0      # Number of correct predictions\n",
    "        total = 0        # Total number of examples\n",
    "        loader = tqdm(loader)\n",
    "        # Loop over all batches in the dataset\n",
    "        for imgs, labels in loader:\n",
    "            # Move data to GPU (or CPU), depending on device\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            # Encode static images into spike trains over time\n",
    "            spikes = encode_batch(imgs, timesteps)  # [timesteps, batch, channels, H, W]\n",
    "\n",
    "            # Reset gradients before backprop\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get predictions from the SNN\n",
    "            outputs = model(spikes)  # [batch, num_classes]\n",
    "\n",
    "            # Compute loss between predictions and true labels\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute gradients, backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            preds = torch.argmax(outputs, dim=1)  # [batch]\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(loader):.4f} | Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Function (Full Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_snn(model, loader):\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    # Disables gradient calculations (saves memory and speeds up inference)\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            # Move data to the same device as the model\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            # Encode static images into spiking format\n",
    "            spikes = encode_batch(imgs, timesteps)\n",
    "            # Run the forward pass to get predictions\n",
    "            outputs = model(spikes)\n",
    "            # Get predicted class index (0-3) with highest score per sample\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Saves results for evaluation, move from GPU to CPU.\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred) * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred, average='weighted') * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall_score(y_true, y_pred, average='weighted') * 100:.2f}%\")\n",
    "    print(f\"F1 Score: {f1_score(y_true, y_pred, average='weighted') * 100:.2f}%\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 179/179 [01:23<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.5908 | Accuracy: 77.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 179/179 [01:22<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Loss: 0.5912 | Accuracy: 77.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 179/179 [01:23<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Loss: 0.5817 | Accuracy: 77.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 179/179 [01:21<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Loss: 0.5814 | Accuracy: 77.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 179/179 [01:22<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Loss: 0.5846 | Accuracy: 77.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 179/179 [01:22<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Loss: 0.5776 | Accuracy: 77.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 179/179 [01:21<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Loss: 0.5780 | Accuracy: 77.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 179/179 [01:19<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Loss: 0.5777 | Accuracy: 77.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 179/179 [01:21<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Loss: 0.5703 | Accuracy: 77.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 179/179 [01:16<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Loss: 0.5742 | Accuracy: 77.99%\n",
      "Accuracy: 77.88%\n",
      "Precision: 79.05%\n",
      "Recall: 77.88%\n",
      "F1 Score: 78.11%\n",
      "Confusion Matrix:\n",
      "[[236  45   1  18]\n",
      " [ 66 200  26  14]\n",
      " [ 39  10 344  12]\n",
      " [ 39   6  14 241]]\n"
     ]
    }
   ],
   "source": [
    "train_snn(model, train_loader, epochs=10)\n",
    "test_snn(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
